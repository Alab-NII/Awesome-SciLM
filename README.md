# Awesome PLM for Scientific Text (SciLM)

## Content
- Related survey papers
- Existing SciLMs
  - Bio SciLMs
  - Chemical SciLMs
  - Multi-domain SciLMs
  - Other domains SciLMs
- Awesome scientific datasets

## Related survey papers
- [A Survey of Large Language Models](https://arxiv.org/abs/2303.18223) - arXiv 2023
- [Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey](https://arxiv.org/abs/2302.10035) - Accepted by Machine Intelligence Research 2023
- [Pre-trained Language Models in Biomedical Domain: A Systematic Survey](https://arxiv.org/abs/2110.05006) - 	Accepted in ACM Computing Surveys 2023
- [AMMU: A survey of transformer-based biomedical pretrained language models](https://dl.acm.org/doi/abs/10.1016/j.jbi.2021.103982) - Journal of Biomedical Informatics 2022
- [Pre-Trained Language Models and Their Applications](https://www.sciencedirect.com/science/article/pii/S2095809922006324) - Engineering, 2022
- [Pre-trained models: Past, present and future](https://www.sciencedirect.com/science/article/pii/S2666651021000231) - AI Open, Volume 2, 2021


## Existing SciLMs

### Bio SciLMs


### Chemical SciLMs


### Multi-domain SciLMs
No. | Year | Name | Base-model | Objective | #Parameters
|---|---| --- |---|---| --- |
1 | 2019/11 | S2ORC-SciBERT | BERT | MLM, NSP | 110M
2 | 2020/04 | SPECTER | BERT | Triple-loss | 110M
3 | 2021/03 | OAG-BERT | BERT | MLM | 110M
4 | 2022/05 | ScholarBERT | BERT | MLM | 770M
5 | 2022/06 | SciDEBERTa | DeBERTa | MLM | N/A
6 | 2022/09 | CSL-T5 | T5 | Fill-in-the-blank-style denoising objective | 220M
7 | 2022/10 | AcademicRoBERTa | RoBERTa | MLM | 125M
8 | 2022/11 | Galactica | GPT | Autoregressive Language Model | "125M
1.3B
6.7B
30B
120B"
9 | 2022/11 | VarMAE | RoBERTa | MLM | 110M
10 | 2023/05 | Patton | GNN + BERT | Network-contextualized MLM, Masked Node Prediction | N/A

### Other domains SciLMs
  
