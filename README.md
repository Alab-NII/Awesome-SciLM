# Awesome PLM for Scientific Text (SciLM)

## Content
- [Related survey papers](https://github.com/Alab-NII/Awesome-SciLM/tree/main#related-survey-papers)
- Existing SciLMs
  - Bio SciLMs
  - Chemical SciLMs
  - Multi-domain SciLMs
  - Other domains SciLMs
- Awesome scientific datasets

## Related survey papers
- [A Survey of Large Language Models](https://arxiv.org/abs/2303.18223) - arXiv 2023
- [Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey](https://arxiv.org/abs/2302.10035) - Accepted by Machine Intelligence Research 2023
- [Pre-trained Language Models in Biomedical Domain: A Systematic Survey](https://arxiv.org/abs/2110.05006) - 	Accepted in ACM Computing Surveys 2023
- [AMMU: A survey of transformer-based biomedical pretrained language models](https://dl.acm.org/doi/abs/10.1016/j.jbi.2021.103982) - Journal of Biomedical Informatics 2022
- [Pre-Trained Language Models and Their Applications](https://www.sciencedirect.com/science/article/pii/S2095809922006324) - Engineering, 2022
- [Pre-trained models: Past, present and future](https://www.sciencedirect.com/science/article/pii/S2666651021000231) - AI Open, Volume 2, 2021


## Existing SciLMs

### Bio SciLMs


### Chemical SciLMs
No. | Year | Name | Base-model | Objective | #Parameters | Code
|---|---| --- |---|---| --- |--- |
1 | 2020/03 | NukeBERT | BERT | MLM, NSP | 110M | Github
2 | 2020/10 | ChemBERTa | RoBERTa | MLM | 125M | Github
3 | 2021/05 | NukeLM | SciBERT, RoBERTa | MLM | 125M, 355M, 110M | Github
4 | 2021/06 | ChemBERT | RoBERTa | MLM | 110M | Github
5 | 2021/09 | MatSciBERT | BERT | MLM | 110M | Github
6 | 2021/10 | MatBERT | BERT | MLM | 110M | Github
7 | 2022/05 | BatteryBERT | BERT, SciBERT | MLM | 110M | Github
8 | 2022/05 | ChemGPT | GPT | Autoregressive Language Model | 1B | Github
9 | 2022/08 | MaterialsBERT (Shetty) | PubMedBERT | MLM, NSP, Whole-Word Masking | 110M | Github
10 | 2022/08 | ProcessBERT | BERT | MLM, NSP | 110M | Github
11 | 2022/09 | ChemBERTa-2 | RoBERTa | MLM, Multi-task Regression | 125M | Github
12 | 2022/09 | MaterialBERT (Yoshitake) | BERT | MLM, NSP | 110M | Github
13 | 2023/08 | GIT-Mol | GIT-Former | Xmodal-Text Matching, Xmodal-Text Contrastive Learning | 700M | Github


### Multi-domain SciLMs
No. | Year | Name | Base-model | Objective | #Parameters | Code
|---|---| --- |---|---| --- |--- |
1 | 2019/11 | S2ORC-SciBERT | BERT | MLM, NSP | 110M | Github
2 | 2020/04 | SPECTER | BERT | Triple-loss | 110M | Github
3 | 2021/03 | OAG-BERT | BERT | MLM | 110M | Github
4 | 2022/05 | ScholarBERT | BERT | MLM | 770M | Github
5 | 2022/06 | SciDEBERTa | DeBERTa | MLM | N/A | Github
6 | 2022/09 | CSL-T5 | T5 | Fill-in-the-blank-style denoising objective | 220M | Github
7 | 2022/10 | AcademicRoBERTa | RoBERTa | MLM | 125M | Github
8 | 2022/11 | Galactica | GPT | Autoregressive Language Model | 125M, 1.3B, 6.7B, 30B, 120B | Github
9 | 2022/11 | VarMAE | RoBERTa | MLM | 110M | Github
10 | 2023/05 | Patton | GNN + BERT | Network-contextualized MLM, Masked Node Prediction | N/A | Github

### Other domains SciLMs
  
